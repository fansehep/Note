### 信息及其度量

引言

- 通信的目的在于传输信息中所包含的信息
- 消息中$\textcolor{Orange}{不确定的内容}$才构成信息.$\textcolor{Orchid}{信息量}$就是对这种不确定性的定量描述
- 信息具有普遍存在性(即存在于任何事物的运动和变化中) : **可扩充** 或 **压缩性**; **可存储** 与 **传输性**; **相对性** ( 不同的认识主体观察到的信息不同) : 可度量, 可共享 和 时效性 ( 信息 具有 ==生命周期==).
- 在当今信息社会中, **信息**是最宝贵的资源之一.

信息必须依附于一定的物质形式存在

同样的信息可用不同的形式的消息来表述

信息是消息的内涵, 消息是信息的外在表现

如何度量信息中包含的信息量 ?

- 原则

  - 可以度量任何消息, 度量方法 与 消息的种类无关

  - 与消息的重要程度无关

    上例表明

    - 消息中所包含的信息量 和 不可预测性 或 不确定性有关
    - 消息所表达的事件越不可能发生, 信息量就越大
    
  - 信息量 I 可用 概率 P 来度量 :  I = f [ P(x) ]
  
- 1. 离散消息 X 的信息量
     - P -> 1, I -> 0
       - 概率趋于 1, 则信息量趋于 0
         - 就是说 越是容易发生的事情，那么他的信息量就越低。
     - P -> 0, I -> 无穷
     - P(x) < P(y)
       - I (x) > I (y)

- I = log a ( 1 / P(x) ) = - log a P ( x )

  - a = 2 (bit)
  - a = e (nat)
  - a = 10 哈特莱

- 常用 I = log 2 (1 / P(x) )

- 二 进制 的每个码元 包含 1 bit

- 四 进制 的每个码元 包含 2 bit

- 推广 : M 进制的每个码元包含 log 2 M bit



